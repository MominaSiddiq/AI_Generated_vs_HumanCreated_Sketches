{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOvwBeQCGy3gkz2I2LMiyZ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MominaSiddiq/AI_Generated_vs_HumanCreated_Sketches/blob/main/Bert_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "Puz7VpXXUP_c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jR0rRi-URYxt"
      },
      "outputs": [],
      "source": [
        "# Install required Hugging Face libraries\n",
        "!pip install -q transformers datasets accelerate\n",
        "\n",
        "# âœ… Upgrade transformers to the latest version to avoid Trainer-related errors\n",
        "!pip install -U transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix for dataset loading issue: upgrade fsspec to latest version\n",
        "!pip install -U fsspec==2023.6.0\n"
      ],
      "metadata": {
        "id": "cFNf3F9NV3rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "a_HwIXrKUWQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import essential libraries for working with transformers and datasets\n",
        "from datasets import load_dataset                    # For loading the IMDb dataset\n",
        "from transformers import (BertTokenizer,             # Tokenizer for BERT\n",
        "                          BertForSequenceClassification,  # Pretrained BERT model for sentiment classification\n",
        "                          Trainer,                   # Trainer handles the training loop\n",
        "                          TrainingArguments)         # Used to define training configurations\n",
        "import torch                                          # PyTorch backend\n"
      ],
      "metadata": {
        "id": "l8qt-M-RUGLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load IMDb Dataset\n",
        "\n",
        "Load the IMDb movie reviews dataset using Hugging Face's `datasets` library. This dataset contains 25,000 labeled movie reviews for training and 25,000 for testing, with binary sentiment labels: `0` for negative, and `1` for positive.\n"
      ],
      "metadata": {
        "id": "2tP9HrFBUdwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDb dataset from Hugging Face\n",
        "# The dataset contains 25,000 training and 25,000 test examples\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Display the dataset structure\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "XfmtALKzUzlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Printed Sample\n",
        "\n",
        "Below, a positive and a negative example from the dataset is printed to better understand the data.\n"
      ],
      "metadata": {
        "id": "GGnh7_irY9Z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of printing full text, just show first 300 characters\n",
        "print(\"Sample Negative Review:\\n\")  # Show a sample of negitive review\n",
        "print(dataset['train'][0]['text'][:300])\n",
        "print(\"Label:\", dataset['train'][0]['label'])\n",
        "\n",
        "print(\"\\nSample Positive Review:\\n\") # Show a sample of positive review\n",
        "print(dataset['train'][1]['text'][:300])\n",
        "print(\"Label:\", dataset['train'][1]['label'])\n",
        "\n"
      ],
      "metadata": {
        "id": "mbuHGruEZJZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing the Dataset\n",
        "\n",
        "The text data is tokenized using a pretrained BERT tokenizer.\n",
        "Each movie review is converted into input tokens and padded or truncated to a fixed length.\n",
        "The tokenizer also generates attention masks, which indicate which tokens are actual input versus padding.\n"
      ],
      "metadata": {
        "id": "Y-dXkWR_Apsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load BERT Tokenizer"
      ],
      "metadata": {
        "id": "VnTxn7CnBSa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained BERT tokenizer (base uncased model)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "metadata": {
        "id": "zZwTNI13BKxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a Tokenization Function"
      ],
      "metadata": {
        "id": "49QMyd8gCecT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that will tokenize the text data\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        padding=\"max_length\",       # pad all sequences to max_length\n",
        "        truncation=True,            # truncate reviews longer than max_length\n",
        "        max_length=512              # BERT supports max 512 tokens\n",
        "    )\n"
      ],
      "metadata": {
        "id": "sIO3YVecCmAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Tokenization to the Dataset"
      ],
      "metadata": {
        "id": "86XNkrpvCpnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply tokenization to the entire dataset\n",
        "# This creates new fields: input_ids, token_type_ids, attention_mask\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "uyAMo8lSCtVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Unused Columns"
      ],
      "metadata": {
        "id": "NhEzKlu_FFK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the original text column to keep only tokenized inputs\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n"
      ],
      "metadata": {
        "id": "IOeZoY-lFJ-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Format for PyTorch"
      ],
      "metadata": {
        "id": "DMM8onkKFNrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the dataset format for PyTorch (input_ids, attention_mask, labels)\n",
        "tokenized_datasets.set_format(\"torch\")\n"
      ],
      "metadata": {
        "id": "Cdv-73EpFRln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug Check"
      ],
      "metadata": {
        "id": "fGxLq-tKFUjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview one tokenized example\n",
        "# Temporarily remove formatting to preview\n",
        "tokenized_datasets.reset_format()\n",
        "print(tokenized_datasets[\"train\"][0])"
      ],
      "metadata": {
        "id": "RG0cqJ14FZPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set it back to torch format\n",
        "tokenized_datasets.set_format(\"torch\")\n"
      ],
      "metadata": {
        "id": "KSG5bRf5Ggz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining and Training the BERT Model\n",
        "\n",
        "A pretrained BERT model (`bert-base-uncased`) is loaded for sequence classification.\n",
        "The model is then fine-tuned on the IMDb movie review dataset using the Hugging Face `Trainer` API.\n",
        "Training arguments such as learning rate, batch size, and number of epochs are defined to control the fine-tuning process.\n"
      ],
      "metadata": {
        "id": "eD2pKDrwIWnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the BERT Model"
      ],
      "metadata": {
        "id": "yOZ3Jlp8I-67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained BERT model for sequence classification with two labels\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
      ],
      "metadata": {
        "id": "dgtE8kf7Is4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training Arguments"
      ],
      "metadata": {
        "id": "-bCGhGq7JNQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training parameters for the Trainer API\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",              # output directory for checkpoints\n",
        "    evaluation_strategy=\"epoch\",         # evaluate every epoch\n",
        "    save_strategy=\"epoch\",               # save model every epoch\n",
        "    per_device_train_batch_size=8,       # batch size for training\n",
        "    per_device_eval_batch_size=8,        # batch size for evaluation\n",
        "    num_train_epochs=2,                  # number of training epochs\n",
        "    learning_rate=2e-5,                  # learning rate\n",
        "    weight_decay=0.01,                   # weight decay to reduce overfitting\n",
        "    logging_dir=\"./logs\",                # directory for logs\n",
        "    logging_steps=10,                    # log every 10 steps\n",
        "    load_best_model_at_end=True          # load best model after training\n",
        ")\n"
      ],
      "metadata": {
        "id": "nrMI6YQqJQQ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}